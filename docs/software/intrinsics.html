<!doctype html>
<html>
  <head>
    <title>Intrinsics</title>
  </head>
  <body>
    <h1>Intrinsics</h1> 
	
	<h2>Assembly in GCC and VC++</h2>
	<table border="1">
    <tbody>
    <tr>
      <td><b>Instructions</b></td>
      <td><b>GCC (AT&T)</b></td>
      <td><b>VC++ (Intel)</b></td>
    </tr>
    <tr>
      <td>Move ebx to eax</td>
      <td>movl %ebx, %eax</td>
      <td>mov eax, ebx</td>
    </tr>
    <tr>
      <td>Move 100 to ebx</td>
      <td>movl $100, %ebx</td>
      <td>mov ebx, 100</td>
    </tr>
    <tr>
      <td>Add xmm2 and xmm3, store in xmm1</td>
      <td>vpaddd %xmm3, %xmm2, %xmm1</td>
      <td>vpaddd xmm1, xmm2, xmm3</td>
    </tr>	
    </tbody>
    </table>
	
	
	
    <h2>Instructions</h2>
		
	<p><a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">IntrinsicsGuide</a></p>
	<p>emmintrin.h: SSE2 CPUID Flag</p>
	<p>immintrin.h: AVX2 CPUID Flag</p>

	
    <table border="1">
    <tbody>
    <tr>
    <td><b>Description</b></td>
	<td><b>Instruction</b></td>
    <td><b>SSE2</b></td>
    <td><b>AVX2</b></td>
    </tr>
    <tr>
      <td>Load 128-bits(256-bits) of integer data</td>
	  <td>movdqa (vmovdqa)</td>
      <td>__m128i _mm_load_si128(__m128i const* mem_addr)</td>
      <td>__m256i _mm256_load_si256(__m256i const * mem_addr)</td>
    </tr>	
    <tr>
      <td>Add packed 32-bit integers</td>
	  <td>paddd (vpaddd)</td>
      <td>__m128i _mm_add_epi32(__m128i a, __m128i b)</td>
      <td>__m256i _mm256_add_epi32(__m256i a, __m256i b)</td>
    </tr> 
    <tr>
      <td>Compute the bitwise NOT of 128(256) bits and then AND</td>
	  <td>pandn (vpandn)</td>
      <td>__m128i _mm_andnot_si128(__m128i a, __m128i b)</td>
      <td>__m256i _mm256_andnot_si256(__m256i a, __m256i b)</td>
    </tr> 
    <tr>
      <td>Compute the bitwise AND of 128(256) bits</td>
	  <td>pand (vpand)</td>
      <td>__m128i _mm_and_si128(__m128i a, __m128i b)</td>
      <td>__m256i _mm256_and_si256(__m256i a, __m256i b)</td>
    </tr> 	
    <tr>
      <td>Compute the bitwise XOR of 128(256) bits</td>
	  <td>pxor (vpxor)</td>
      <td>__m128i _mm_xor_si128(__m128i a, __m128i b)</td>
      <td>__m256i _mm256_xor_si256(__m256i a, __m256i b)</td>
    </tr> 		
    <tr>
      <td>Shift packed 32-bit integers left</td>
	  <td>pslld (vpslld)</td>
      <td>__m128i _mm_slli_epi32(__m128i a, int imm8)</td>
      <td>__m256i _mm256_slli_epi32(__m256i a, int imm8)</td>
    </tr> 		
    <tr>
      <td>Shift packed 32-bit integers right</td>
	  <td>psrld (vpsrld)</td>
      <td>__m128i _mm_srli_epi32(__m128i a, int imm8)</td>
      <td>__m256i _mm256_srli_epi32(__m256i a, int imm8)</td>
    </tr> 	
    <tr>
      <td>Broadcast 32-bit integer</td>
	  <td>Sequence</td>
      <td>__m128i _mm_set1_epi32(int a)</td>
      <td>__m256i _mm256_set1_epi32(int a)</td>
    </tr> 
    <tr>
      <td>Shuffle 32-bit integers (within 128-bit lanes)</td>
	  <td>pshufd (vpshufd)</td>
      <td>__m128i _mm_shuffle_epi32(__m128i a, int imm8)</td>
      <td>__m256i _mm256_shuffle_epi32(__m256i a, const int imm8)</td>
    </tr> 
    <tr>
      <td>Copy 128 bits of integer data and the remaining values</td>	  
	  <td>- (vinserti128)</td>
      <td>-</td>
      <td>__m256i _mm256_inserti128_si256(__m256i a, __m128i b, const int imm8)</td>
    </tr> 
    <tr>
      <td>Blend packed 32-bit integers</td>
	  <td>- (vpblendd)</td>
      <td>-</td>
      <td>__m256i _mm256_blend_epi32(__m256i a, __m256i b, const int imm8)</td>
    </tr> 	
	
	
	
    <tr>
      <td></td>
	  <td></td>
      <td></td>
      <td></td>
    </tr> 		
    </tbody>
    </table>
  </body>
</html>
